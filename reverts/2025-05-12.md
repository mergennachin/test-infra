# Week of 2025-05-12 to 2025-05-19 (29)

### GHFirst (19)

- [Revert "[Dynamo] added warning message for tracing lru_cache wrapped functions (#153744)"](https://github.com/pytorch/pytorch/commit/75eb2f3ff6c5a7d8004465935226d7449444e491)
  - Need to revert as it is breaking internal signals: [D74935585](https://www.internalfb.com/diff/D74935585) ([comment](https://github.com/pytorch/pytorch/pull/153744#issuecomment-2889187038))
- [Revert "[BE]: Enable RUFF TRY400 rule - log.exception (#153473)"](https://github.com/pytorch/pytorch/commit/3443627e078deb813ae37f7182d41a802bd05ac4)
  - seems to have broken internal signals, @albanD may I count on you to help the author merge his PR? D74837988 ([comment](https://github.com/pytorch/pytorch/pull/153473#issuecomment-2886017075))
- [Revert "[Ez][BE]: Remove accidental classvar (#153540)"](https://github.com/pytorch/pytorch/commit/86c6f71ddbdea3ca61635b810ce159b4aed9fa4d)
  - Broken internal tests, @albanD may you help the author get his PR merged? D74804063 ([comment](https://github.com/pytorch/pytorch/pull/153540#issuecomment-2886011101))
- [Revert "[inductor][dynamo] Include operator name in size/stride/alignment assertion (#152353)"](https://github.com/pytorch/pytorch/commit/4d073af58ca8669c6a675b4acfd1b689145def1b)
  - seems to have broken a few internal tests, @jansel may you help the author get his PR merged? ([comment](https://github.com/pytorch/pytorch/pull/152353#issuecomment-2885997862))
- [Revert "[cuDNN][SDPA] cuDNN SDPA refactor/cleanup, nested tensor backward, test priority bump for `sm90`, `sm100` (#149282)"](https://github.com/pytorch/pytorch/commit/f363a3f51ab3ec3c76c08c62c2d63f1b77738500)
  - Breaking internal builds, see [D74729259](https://www.internalfb.com/diff/D74729259). @drisspg may you help out the author have their PR merged? ([comment](https://github.com/pytorch/pytorch/pull/149282#issuecomment-2881546951))
- [Revert "[ROCm] Maxpool forward NHWC Perf Improvement targeting Resnet scenarios (#151727)"](https://github.com/pytorch/pytorch/commit/6ef1cbc191077a2434be8297ba91a1bc8c4aa4f4)
  - Seems to be breaking internal builds, @seemethere may you help the author? [D74729252](https://www.internalfb.com/diff/D74729252) ([comment](https://github.com/pytorch/pytorch/pull/151727#issuecomment-2881122917))
- [Revert "Fix skipIfXpu and skipIfHpu disables tests when used on class (#151315)"](https://github.com/pytorch/pytorch/commit/2344eca5ebd378c2cd1e8c0373b9926da23baf8a)
  - Seems to have introduced internal regressions, see [D74668899](https://www.internalfb.com/diff/D74668899). @malfet may you help the author get this PR merged? ([comment](https://github.com/pytorch/pytorch/pull/151315#issuecomment-2880203323))
- [Revert "Rewrite autograd producer consumer stream sync logic (#151079)"](https://github.com/pytorch/pytorch/commit/2c1912452d7c62fe5519f30d0f621b3179306409)
  - Seems to have introduced regressions in internal signals, see [D74648937](https://www.internalfb.com/diff/D74648937) ([comment](https://github.com/pytorch/pytorch/pull/151079#issuecomment-2880176879))
- [Revert "Enable accelerator to perform streaming backward (#153412)"](https://github.com/pytorch/pytorch/commit/a628efd1e8ee9fcd6e3b871754a998568806b176)
  - Need to revert in order to revert https://github.com/pytorch/pytorch/pull/151079 ([comment](https://github.com/pytorch/pytorch/pull/153412#issuecomment-2880169739))
- [Revert "[DSD] Don't pop tensors if they are on Meta device (#153185)"](https://github.com/pytorch/pytorch/commit/8d7dec6e92211df33a3861b031da81bcae023342)
  - Seems to break internal signals, see [D74577069](https://www.internalfb.com/diff/D74577069) ([comment](https://github.com/pytorch/pytorch/pull/153185#issuecomment-2875662357))
- [Revert "[Hierarchical Compilation] Track node mutations (#152389)"](https://github.com/pytorch/pytorch/commit/5c3fddb9cca478c7a32b7103e92e5682e8554c45)
  - Humm, interesting, there seems to be a bug in stack PRs, as it should be part of the stack and be reverted with the other ones ([comment](https://github.com/pytorch/pytorch/pull/152389#issuecomment-2873540451))
- [Revert "[Hierarchical Compilation] Use universal flatten APIs (#152505)"](https://github.com/pytorch/pytorch/commit/78d752e96a0d9aa213813be185cb03833c73746d)
  - [TENTATIVE] reverting to check if reverting this stack partially caused the introduction of https://github.com/pytorch/pytorch/actions/runs/14966121510/job/42049638969#step:22:875 ([comment](https://github.com/pytorch/pytorch/pull/152505#issuecomment-2872869990))
- [Revert "[ROCm] Maxpool forward NHWC Perf Improvement targeting Resnet scenarios (#151727)"](https://github.com/pytorch/pytorch/commit/daca611465c93ac6b8147e6b7070ce2b4254cfc5)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/151727#issuecomment-2872361816))
- [Revert "Forward fix #151727 (#153306)"](https://github.com/pytorch/pytorch/commit/8511d210819dd3be3938462f8cb6d1b7118adbc8)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/153306#issuecomment-2872339570))
- [Revert "[Hierarchical Compile] Add mutation dependencies to topological sorting (#152410)"](https://github.com/pytorch/pytorch/commit/47df195065d62a649169565293b24595249ca3b1)
  - Breaking internal signal citadel-fbcode-test-mode-opt-for-pt2_stack_for_internal-linux-0 please see diff [D74531503](https://www.internalfb.com/diff/D74531503) for more details ([comment](https://github.com/pytorch/pytorch/pull/152410#issuecomment-2871168679))
- [Revert "[Hierarchical Compile] Take into account mutation deps in cycle detection (#152506)"](https://github.com/pytorch/pytorch/commit/0e36887209be8fe3dbfbb82199fae5127787f07f)
  - Breaking internal signal citadel-fbcode-test-mode-opt-for-pt2_stack_for_internal-linux-0 please see diff [D74531503](https://www.internalfb.com/diff/D74531503) for more details ([comment](https://github.com/pytorch/pytorch/pull/152410#issuecomment-2871168679))
- [Revert "[Hierarchical Compile] Replace tracing alias and mutation check with dynamo impl (#152570)"](https://github.com/pytorch/pytorch/commit/53ebcabb527f6b440e407068afe84726e657b8b7)
  - Breaking internal signal citadel-fbcode-test-mode-opt-for-pt2_stack_for_internal-linux-0 please see diff [D74531503](https://www.internalfb.com/diff/D74531503) for more details ([comment](https://github.com/pytorch/pytorch/pull/152410#issuecomment-2871168679))
- [Revert "[Dynamo] Fix typing in graph_deduplication.py (#152572)"](https://github.com/pytorch/pytorch/commit/0071fdab9ee81434e27e74174be98006f3cd175a)
  - Breaking internal signal citadel-fbcode-test-mode-opt-for-pt2_stack_for_internal-linux-0 please see diff [D74531503](https://www.internalfb.com/diff/D74531503) for more details ([comment](https://github.com/pytorch/pytorch/pull/152410#issuecomment-2871168679))
- [Revert "[Dynamo] Optimize dedupe region ancestor tracking (#152589)"](https://github.com/pytorch/pytorch/commit/aa7fe6af4165dcecd19c3b00e21c56edc22b714f)
  - Breaking internal signal citadel-fbcode-test-mode-opt-for-pt2_stack_for_internal-linux-0 please see diff [D74531503](https://www.internalfb.com/diff/D74531503) for more details ([comment](https://github.com/pytorch/pytorch/pull/152410#issuecomment-2871168679))

### Ignored Signal (2)

- [Revert "Reapply "Delete TorchScript based Android demo app and point to ExecuTorch (#153633)" (#153656)"](https://github.com/pytorch/pytorch/commit/084c4aa6140fa3e3ae66b09ffe893df841fa06da)
  - Still being used internally so can't remove ([comment](https://github.com/pytorch/pytorch/pull/153656#issuecomment-2887665403))
- [Revert "Delete TorchScript based Android demo app and point to ExecuTorch (#153633)"](https://github.com/pytorch/pytorch/commit/ae0e8f0c7316addab3f415dc767a9d34f58b0dae)
  - But libtorch build regressions are real, fbjni is still used for C++ builds ([comment](https://github.com/pytorch/pytorch/pull/153633#issuecomment-2884951805))

### No Signal (8)

- [Revert "[CUDA][cuBLAS][cuBLASLt] avoid polluting prefer cuBLAS/Lt setting across tests (#153655)"](https://github.com/pytorch/pytorch/commit/40339c1e997ce1ba2e649015ebd4ec217ccbc8e3)
  - Sorry for reverting your change but it seems to fail a test in trunk ([comment](https://github.com/pytorch/pytorch/pull/153655#issuecomment-2888212597))
- [Revert "cleanup, refactor and add missing  self._dde_suppressed checks (#152657)"](https://github.com/pytorch/pytorch/commit/1748fa529a93f4aa7b69998ee5387bc91b653502)
  - Broke lint ([comment](https://github.com/pytorch/pytorch/pull/152657#issuecomment-2887539146))
- [Revert "[dynamo] Make `OptimizedModule` more robust in attribute reads and writes (#153637)"](https://github.com/pytorch/pytorch/commit/c2dda47bc5929a0797a49cc9216be2c68165b2a3)
  - Looks like it broke slow tests, see https://hud.pytorch.org/hud/pytorch/pytorch/cda572b053033abc57b3b3358a861cbc71a490b9/1?per_page=50&name_filter=&mergeEphemeralLF=true ([comment](https://github.com/pytorch/pytorch/pull/153637#issuecomment-2887449037))
- [Revert "[ca][dynamo] always run eager checkpoint region's recomputation in eager (#153300)"](https://github.com/pytorch/pytorch/commit/236b08cbf83edcd168aaa384e2603dbf8d2acc2f)
  - Looks like it breaks rocm, see https://hud.pytorch.org/hud/pytorch/pytorch/fa8543454ab5d3deda1d15c1f8d24e9ebe14f340/1?per_page=50&name_filter=slow%20%2F%20linux-jammy-rocm&mergeEphemeralLF=true ([comment](https://github.com/pytorch/pytorch/pull/153300#issuecomment-2884489459))
- [Revert "[ca][dtensor] run real PG dtensor tests under CA (#152689)"](https://github.com/pytorch/pytorch/commit/2327c9eedcc0fa673dc183fe9393858ee34b4517)
  - Looks like it breaks rocm, see https://hud.pytorch.org/hud/pytorch/pytorch/fa8543454ab5d3deda1d15c1f8d24e9ebe14f340/1?per_page=50&name_filter=slow%20%2F%20linux-jammy-rocm&mergeEphemeralLF=true ([comment](https://github.com/pytorch/pytorch/pull/153300#issuecomment-2884489459))
- [Revert "[FlexAttention] Enforce Q,K,V memory layouts for fp8 flex attention to avoid perf degradation (#153357)"](https://github.com/pytorch/pytorch/commit/71027b13b235e9265907cc0f3a71fb51b29e33aa)
  - Might have introduced regressions in rocm testing for main: https://github.com/pytorch/pytorch/actions/runs/15035410497/job/42257000513 feel free to re-merge if this was a mistake ([comment](https://github.com/pytorch/pytorch/pull/153357#issuecomment-2882915691))
- [Revert "[CUDA][CUDNN] Dispatch to cuDNN for non-batch-splittable 64-bit NCHW convolutions (#153101)"](https://github.com/pytorch/pytorch/commit/bf0fe4f82875d1ddce54d5de34a8589728cbb967)
  - Seems to have introduced breakages on main, tentative revert: https://github.com/pytorch/pytorch/actions/runs/15024667248/job/42224521705 ([comment](https://github.com/pytorch/pytorch/pull/153101#issuecomment-2881208171))
- [Revert "[export][cond] support merging constant ints as unbacked symint (#152742)"](https://github.com/pytorch/pytorch/commit/641e4bee67453199ca5144745ddab969d9934e9f)
  - breaking trunk ([comment](https://github.com/pytorch/pytorch/pull/152742#issuecomment-2874410372))
