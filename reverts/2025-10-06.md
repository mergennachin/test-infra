# Week of 2025-10-06 to 2025-10-13 (44)

### Auto Revert (6)

- [Revert "Enable ruff rule E721 (#165162)"](https://github.com/pytorch/pytorch/commit/816fb7f48d121bea96f6e415bdf63e2538490cfb)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/165162#issuecomment-3393328271))
- [Revert "[CD] Do not propagate download.pytorch.org IP into container (#165075)"](https://github.com/pytorch/pytorch/commit/daea35df5c11d893f37113cf0949a464966f7424)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/165075#issuecomment-3388228013))
- [Revert "AOTI MPS Shim Implementation (#163865)"](https://github.com/pytorch/pytorch/commit/4412026949b562f940d4c24162de19d299725b62)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/163865#issuecomment-3385196387))
- [Revert "Do not decompose in functionalization/proxy tensor if autograd wouldn't have decomposed (#164939)"](https://github.com/pytorch/pytorch/commit/06d86e58d0309aa2c217256f88d1990a22ec6e4f)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/164939#issuecomment-3385056722))
- [Revert "Fix truediv numerics between eager and compile (#164144)"](https://github.com/pytorch/pytorch/commit/e09fb44ef177005c4a11c28be24781429d416a3e)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/164144#issuecomment-3384769092))
- [Revert "Fix Avoid DDE in item numel check (#164934)"](https://github.com/pytorch/pytorch/commit/5209c8ce0704f34ba4bd2a58c19877fbf6cf0392)
  - Reverted automatically by pytorch's autorevert, to avoid this behaviour add the tag autorevert: disable ([comment](https://github.com/pytorch/pytorch/pull/164934#issuecomment-3384390621))

### GHFirst (8)

- [Revert "Do not decompose in functionalization/proxy tensor if autograd wouldn't have decomposed (#164939)"](https://github.com/pytorch/pytorch/commit/5c3fe9fb302c68215e1c39d84559aa54b4285304)
  - introduces numeric issues internally, see [D84326613](https://www.internalfb.com/diff/D84326613) ([comment](https://github.com/pytorch/pytorch/pull/164939#issuecomment-3392203314))
- [Revert "C++ API handle optimizer defaults  (#161825)"](https://github.com/pytorch/pytorch/commit/b67785d9ebda19fab4e49e057254ad7bae0b3a94)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/161825#issuecomment-3391506427))
- [Revert "[export] Turn on install_free_tensors flag (#164691)"](https://github.com/pytorch/pytorch/commit/34ac9b61cbfcf17328ccb8b729509829447fdddd)
  - breaks tests internally, author asked to revert, see [D84230990](https://www.internalfb.com/diff/D84230990) ([comment](https://github.com/pytorch/pytorch/pull/164691#issuecomment-3387718323))
- [Revert "[BC-Breaking] Remove long-deprecated casting functions from native_functions.yaml (#164641)"](https://github.com/pytorch/pytorch/commit/3d1fa40ae1fee18ddf3dca89229e3ae828589e0c)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/164641#issuecomment-3386346474))
- [Revert "[dynamo] Support torch.fx.traceback.annotate (#164678)"](https://github.com/pytorch/pytorch/commit/3040a5d294bd30d3938d0043a5d93d6c23264827)
  - breaks executorch internally, see [D84068062](https://www.internalfb.com/diff/D84068062?entry_point=16) ([comment](https://github.com/pytorch/pytorch/pull/164678#issuecomment-3379281844))
- [Revert "multimem reduce (#164517)"](https://github.com/pytorch/pytorch/commit/f505caa71bd2e4d1e708e20a3665b834134e08fc)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/164517#issuecomment-3378529654))
- [Revert "Fix refine_ranges corner case (#164075)"](https://github.com/pytorch/pytorch/commit/3912ba3e940b9354622fa09b2ada677cd10723d8)
  - fails executorch builds, see [D83938444](https://www.internalfb.com/diff/D83938444) ([comment](https://github.com/pytorch/pytorch/pull/164075#issuecomment-3374430964))
- [Revert "[dynamo] Support torch.fx.traceback.annotate (#164678)"](https://github.com/pytorch/pytorch/commit/cfc5cc17dc4fa6be41b4b31eb6e63d3863479452)
  - fails inductor:max_autotune tests internally, see D83948169 ([comment](https://github.com/pytorch/pytorch/pull/164678#issuecomment-3374407009))

### Ignored Signal (3)

- [Revert "Hotfix test scaled matmul cuda (#165104)"](https://github.com/pytorch/pytorch/commit/7ab00c7c17833d7678e5bd394db2bf8e5f57f7ec)
  - Looks like it broke cuda tests, isn't it, see https://hud.pytorch.org/hud/pytorch/pytorch/44b1ff54e9d6d790293ab49e3d6bd1173b02ff08/1?per_page=50&name_filter=linux-jammy-cuda12.8-py3.10-gcc11%20%2F%20test&mergeEphemeralLF=true ([comment](https://github.com/pytorch/pytorch/pull/165104#issuecomment-3388247886))
- [Revert "[vllm hash update] update the pinned vllm hash (#164628)"](https://github.com/pytorch/pytorch/commit/5b8174bc286725f9326fba6dc0ef17c316486bbd)
  - There are some broken vLLM tests ([comment](https://github.com/pytorch/pytorch/pull/164628#issuecomment-3384560957))
- [Revert "Reland vision pinned commit hash update (#164492)"](https://github.com/pytorch/pytorch/commit/1927783aa3ad676db6f4c34fc77ef3825a4e2ed5)
  - see autorevert msg above, inductor breakage is legit ([comment](https://github.com/pytorch/pytorch/pull/164492#issuecomment-3379537888))

### Landrace (1)

- [Revert "[CUDA][cuBLAS] addmm -- some refactoring for easier navigation between the Lt and non-Lt paths (#163955)"](https://github.com/pytorch/pytorch/commit/f79e212733ca89ce3cc99a3072e50351686e5568)
  - broke on cuda and rocm after landing though this PR had a clean signal initially ([comment](https://github.com/pytorch/pytorch/pull/163955#issuecomment-3386127145))

### No Signal (20)

- [Revert "[compile] Regional inductor compilation with fx.annotate (#164776)"](https://github.com/pytorch/pytorch/commit/8d49cd5b26278bf0b997a42f07d5e24e923576cd)
  - Looks like this one broke everything, not the top of the stack ([comment](https://github.com/pytorch/pytorch/pull/164776#issuecomment-3393725466))
- [Revert "[dynamo][annotate] Remove the need of external ctx mgr of preserve_node_meta (#165188)"](https://github.com/pytorch/pytorch/commit/a19123b37e5658e43e11aa713e5e0ba77c515f53)
  - Looks like it broke bunch of tests, see https://hud.pytorch.org/hud/pytorch/pytorch/2d4654d208394e4ccf5bb071cfb50d7a28265b04/1?per_page=50&mergeEphemeralLF=true ([comment](https://github.com/pytorch/pytorch/pull/165188#issuecomment-3393674273))
- [Revert "[dynamo][executorch] Do not trace into exeuctorch LoweredBackendModule (#165126)"](https://github.com/pytorch/pytorch/commit/d16627f4d0ae02f3b1b00b785c9245ba8fcdd2c0)
  - https://github.com/pytorch/pytorch/pull/165172 is the right way ([comment](https://github.com/pytorch/pytorch/pull/165126#issuecomment-3391975498))
- [Revert "Warn if AccumulateGrad stream does not match producer node stream (#165065)"](https://github.com/pytorch/pytorch/commit/f975bd58af0e8dc4a71001b71a75a361c20a6203)
  - breaks lint ([comment](https://github.com/pytorch/pytorch/pull/165065#issuecomment-3391387386))
- [Revert "[AMP][Refactor] Simplify dtype support logic in autocast context manager (#163446)"](https://github.com/pytorch/pytorch/commit/9420944033a5e74feef53597e5e7804c8b680cb0)
  - breaks autocast tests on linux and mac ([comment](https://github.com/pytorch/pytorch/pull/163446#issuecomment-3390688642))
- [Revert "[2/N] More ruff SIM fixes (#165031)"](https://github.com/pytorch/pytorch/commit/b8be796a57f347df11c7eec7e4ea364780328318)
  - One of the changed line started to fail on trunk ([comment](https://github.com/pytorch/pytorch/pull/165031#issuecomment-3390190870))
- [Revert "[inductor] verify determinism with inductor benchmark script (#164904)"](https://github.com/pytorch/pytorch/commit/d2cb1833445e53d401c3aa9362d96dac1f5e2914)
  - Sorry for reverting your PR but there seems to be some failed vLLM failures coming out of this ([comment](https://github.com/pytorch/pytorch/pull/164904#issuecomment-3388443678))
- [Revert "[inductor][eazy] change how torch.use_deterministic_algorithms affect inductor (#164905)"](https://github.com/pytorch/pytorch/commit/df514a6d5a41c6df8193c9f2e3bf0989d4351bb8)
  - Sorry for reverting your PR but there seems to be some failed vLLM failures coming out of this ([comment](https://github.com/pytorch/pytorch/pull/164905#issuecomment-3388258660))
- [Revert "[ATen] Fix CUDA reduction warp shuffle order (#164790)"](https://github.com/pytorch/pytorch/commit/4d7f9f3aed68729380730ed46e29ff2052f05b73)
  - broke cuda and rocm ci ([comment](https://github.com/pytorch/pytorch/pull/164790#issuecomment-3387558806))
- [Revert "Fix truediv numerics between eager and compile (#164144)"](https://github.com/pytorch/pytorch/commit/ed2d514ad860229f6d364688f9db27dad034cd83)
  - Not sure if it's related, but looks it triggered fuzzer compiler test failure, see https://hud.pytorch.org/hud/pytorch/pytorch/a2f29bcd6388acdc3202d8a90974c50ffb605104/1?per_page=50&name_filter=trunk%20%2F%20linux-jammy-cuda&mergeEphemeralLF=true ([comment](https://github.com/pytorch/pytorch/pull/164144#issuecomment-3387288464))
- [Revert "Enable mimalloc on non-Windows platforms and make default for AArch64 builds (#164741)"](https://github.com/pytorch/pytorch/commit/688efd9741dbd18c176729aec3df7a73825f8463)
  - But it breaks MacOS builds, see https://github.com/pytorch/pytorch/actions/runs/18382886648/job/52373781138 ([comment](https://github.com/pytorch/pytorch/pull/164741#issuecomment-3386859778))
- [Revert "[Code Clean] Remove support of python3.9 (#163846)"](https://github.com/pytorch/pytorch/commit/91040f49348646d79c6cd3434c34860d25c2e47a)
  - breaks distributed tests ([comment](https://github.com/pytorch/pytorch/pull/163846#issuecomment-3386855437))
- [Revert "Use runner with more memory for ASAN builds (#165000)"](https://github.com/pytorch/pytorch/commit/a753ffa9aff47e005c31d6bcbf5b6a61cc54afed)
  - not sure how, but this broke lint ([comment](https://github.com/pytorch/pytorch/pull/165000#issuecomment-3384286412))
- [Revert "list_stored_sd_metadata API. (#160610)"](https://github.com/pytorch/pytorch/commit/fd4bde430a51e5f216295c950d962c6343119821)
  - broke ROCm CI, but flaky also on CUDA CI https://hud.pytorch.org/failure?name=periodic%20%2F%20linux-jammy-rocm-py3.10%20%2F%20test%20(distributed%2C%202%2C%203%2C%20linux.rocm.gpu.mi250.4%2C%20module%3Arocm%2C%20oncall%3Adistributed)&jobName=undefined&failureCaptures=distributed%2Fcheckpoint%2Ftest_list_stored_state_dict.py%3A%3ATestListStateDict%3A%3Atest_list_stored_sd_metadata ([comment](https://github.com/pytorch/pytorch/pull/160610#issuecomment-3382023022))
- [Revert "fix flex attention eager bwd: more rounding (#164317)"](https://github.com/pytorch/pytorch/commit/20082d713666fa1eade588bebd523d86309bfa25)
  - inductor/test_flex_attention.py::TestFlexAttentionCUDA::test_builtin_score_mods_seqlen_lt_custom_sparse_block_size_score_mod4_cuda_float16 [GH job link](https://github.com/pytorch/pytorch/actions/runs/18330774537/job/52207370954) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/41808b2ba9a61ab2f4c7af394c1668d09a4a0331) ([comment](https://github.com/pytorch/pytorch/pull/164317#issuecomment-3381812090))
- [Revert "Fix double dispatch to Python for detach (#163671)"](https://github.com/pytorch/pytorch/commit/97463d4cf3c125557ef23502772b12a67dac4dc7)
  - breaks export tests ([comment](https://github.com/pytorch/pytorch/pull/163671#issuecomment-3379281422))
- [Revert "Enable all flake8-logging-format rules (#164655)"](https://github.com/pytorch/pytorch/commit/f713abab16cb98c15f486e9822dd261279cce252)
  - Looks like it broke lint in trunk, see https://hud.pytorch.org/hud/pytorch/pytorch/bd3b98a8a5d68ddc84b20a4609b9ea90998bf95b/1?per_page=50&name_filter=lintrunner-noclang&mergeEphemeralLF=true ([comment](https://github.com/pytorch/pytorch/pull/164655#issuecomment-3379209309))
- [Revert "[CUDA] Add experimental green context support for SM carveout (#159104)"](https://github.com/pytorch/pytorch/commit/1e42fde45eff81845f269e8185f54a19f6d87c5b)
  - Breaks Windows CD build ([comment](https://github.com/pytorch/pytorch/pull/159104#issuecomment-3378675515))
- [Revert "Fix mesh.get_local_rank when it is > 1d (#164473)"](https://github.com/pytorch/pytorch/commit/afee8062d511ad63e0af65ffac0e712d86aae8f1)
  - appears to be causing vision_maskrcnn regression ([comment](https://github.com/pytorch/pytorch/pull/164473#issuecomment-3374738997))
- [Revert "Numpy zerotensor handling (#164487)"](https://github.com/pytorch/pytorch/commit/1fc71d1b578badb1b3ba7cc2d5795f4f80463749)
  - Did it break torchbench?, see https://hud.pytorch.org/hud/pytorch/pytorch/8c728e129d0f0c69bf6771936080b668a1498397/1?per_page=50&name_filter=inductor_torchbench&mergeEphemeralLF=true ([comment](https://github.com/pytorch/pytorch/pull/164487#issuecomment-3374635051))

### Weird (6)

- [Revert "Fix truediv numerics between eager and compile (#164144)"](https://github.com/pytorch/pytorch/commit/abb2f7179eeaf6d598d709542dd597149f1474a8)
  - It breaks CI again, why was it landed for 3 times in a row without any changes? ([comment](https://github.com/pytorch/pytorch/pull/164144#issuecomment-3390973016))
- [Revert "Add SVE128 ISA (#158932)"](https://github.com/pytorch/pytorch/commit/7614338b69481d702c9f084ac15d9109c7cd3ef0)
  - Hmm, but from OSS point of view, this is a no-op ([comment](https://github.com/pytorch/pytorch/pull/158932#issuecomment-3387961238))
- [Revert "Call internal log_compilation_event if it exists (#164855)"](https://github.com/pytorch/pytorch/commit/47956196d99166fe9083beb2a52fd2e6c90b2011)
  - We should not land this kind of code in core ([comment](https://github.com/pytorch/pytorch/pull/164855#issuecomment-3387692988))
- [Revert "Limit path search within range (#164581)"](https://github.com/pytorch/pytorch/commit/b5e93ffdcf779c703af5c8119636b01f250eafcd)
  - merge sets makes this trickier ([comment](https://github.com/pytorch/pytorch/pull/164581#issuecomment-3381955240))
- [Revert "Add memory estimator (#164738)"](https://github.com/pytorch/pytorch/commit/f8d0d65ddc61e1b2b151127b14738458748f9163)
  - merge sets makes this trickier ([comment](https://github.com/pytorch/pytorch/pull/164581#issuecomment-3381955240))
- [Revert "Reapply "C++-accessible Placements via pybind11 (#163030)" (#164519)"](https://github.com/pytorch/pytorch/commit/df640df68a5275684eaae3080a9c97a0c61469c8)
  - Still breaks internal workflows ([comment](https://github.com/pytorch/pytorch/pull/164519#issuecomment-3378469432))
