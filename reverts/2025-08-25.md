# Week of 2025-08-25 to 2025-09-01 (52)

### GHFirst (23)

- [Revert "[2/N][SymmMem] Add MemPool allocator and tests (#161471)"](https://github.com/pytorch/pytorch/commit/fb2d5ea697a72301d0fb889ead412c6b5ed0d1b8)
  - Multiple internal failures on PR #https://github.com/pytorch/pytorch/pull/161471 will need to land it via co-dev ([comment](https://github.com/pytorch/pytorch/pull/161471#issuecomment-3239283585))
- [Revert "[3/N][SymmMem] Expose offset field from handle (#161532)"](https://github.com/pytorch/pytorch/commit/2e1345a0f8427ecf4eabfc1e3aa1b46787c47467)
  - Multiple internal failures on PR #https://github.com/pytorch/pytorch/pull/161471 will need to land it via co-dev ([comment](https://github.com/pytorch/pytorch/pull/161532#issuecomment-3239282308))
- [Revert "[4/N][SymmMem] Add `get_remote_tensor` + move up `get_buffer` and `get_signal_pad` (#161533)"](https://github.com/pytorch/pytorch/commit/684ae48c160364ea46c77050a7fa24c13a751df2)
  - Multiple internal failures on PR #[161471](https://github.com/pytorch/pytorch/pull/161471) will need to land it via co-dev ([comment](https://github.com/pytorch/pytorch/pull/161533#issuecomment-3239278635))
- [Revert "Ensure large tensor int32 -> int64 indexing is enabled (#157767)"](https://github.com/pytorch/pytorch/commit/ef0483d74c2e7a350c1183eeceb96a3493d4c311)
  - need to revert https://github.com/pytorch/pytorch/pull/157767 internal tests ([comment](https://github.com/pytorch/pytorch/pull/157767#issuecomment-3233558168))
- [Revert "Remove test since it ooms on CI (#161644)"](https://github.com/pytorch/pytorch/commit/5432966253ce1cfafdd4b498b0e92760bf7dbb13)
  - need to revert https://github.com/pytorch/pytorch/pull/157767 internal tests ([comment](https://github.com/pytorch/pytorch/pull/161644#issuecomment-3233550883))
- [Revert "[dynamic shapes] use prims_common contiguity in create_example_tensors (#160933)"](https://github.com/pytorch/pytorch/commit/fa7625660302590b9e08859eda763659ba071ba5)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/160933#issuecomment-3232305708))
- [Revert "[Inductor] Add DeviceAssert op to enable device-side assertion in torch.compile (#160677)"](https://github.com/pytorch/pytorch/commit/c55bdb26e19b96bb9863f0265370e4997254a409)
  - new test is failing internally ([comment](https://github.com/pytorch/pytorch/pull/160677#issuecomment-3230152168))
- [Revert "[2/N][SymmMem] Add MemPool allocator and tests (#161471)"](https://github.com/pytorch/pytorch/commit/903181bb6f1525560e71af08e5a21ed4d79af13c)
  - failing internal builds ([comment](https://github.com/pytorch/pytorch/pull/161471#issuecomment-3230069186))
- [Revert "[3/N][SymmMem] Expose offset field from handle (#161532)"](https://github.com/pytorch/pytorch/commit/8fc2467fe5e9ec01190575e52d4f3f8873562417)
  - need to revert https://github.com/pytorch/pytorch/pull/161471 internal failure ([comment](https://github.com/pytorch/pytorch/pull/161532#issuecomment-3230016806))
- [Revert "Add inductor backend to device interface; make minifier_tests more device agnostic (#151314)"](https://github.com/pytorch/pytorch/commit/014b98dd09babbefd375662f6f14ad88cacb884f)
  - sorry change is faling internally ([comment](https://github.com/pytorch/pytorch/pull/151314#issuecomment-3229774015))
- [Revert "Updates to CuTe DSL template renderer (#161117)"](https://github.com/pytorch/pytorch/commit/38ed57d4465b1d88ca07b8faa7fcc96a08cac22f)
  - will need to revert to unblock revert of https://github.com/pytorch/pytorch/pull/151314 ([comment](https://github.com/pytorch/pytorch/pull/161117#issuecomment-3229754295))
- [Revert "Back out "Refactor CUDAAllocatorConfig to reuse AcceleratorAllocatorConfig (#150312)" (#161002)"](https://github.com/pytorch/pytorch/commit/69c7b16e6fdfa7bce603037b7950b2471b3b2f83)
  - This PR breaks CI TestCudaMallocAsync::test_allocator_settings ([comment](https://github.com/pytorch/pytorch/pull/161002#issuecomment-3228980897))
- [Revert "[Inductor] Update Outer Reduction Heuristic (#159093)"](https://github.com/pytorch/pytorch/commit/4e630f0629d12ab6e550cc6951007c22cd001691)
  - Addressing internal implications then relanding ([comment](https://github.com/pytorch/pytorch/pull/159093#issuecomment-3225942525))
- [Revert "[dynamo, nested graph breaks] add nested graph break tests (#144516)"](https://github.com/pytorch/pytorch/commit/6686974ddd7a616652d9f3dea9195ba8d6d02769)
  - failing internal tests ([comment](https://github.com/pytorch/pytorch/pull/144516#issuecomment-3225659358))
- [Revert "[dynamo, nested graph breaks] support very simple nested graph breaks (#159329)"](https://github.com/pytorch/pytorch/commit/a4fb65701b1b6408a949529442f26248b40f139b)
  - failing internally ([comment](https://github.com/pytorch/pytorch/pull/159329#issuecomment-3225617445))
- [Revert "[dynamo, nested graph breaks] support nested graph breaks x context managers (#159678)"](https://github.com/pytorch/pytorch/commit/6afd766401659e0ba40589939c2576a29e9c64e7)
  - failing internal tests ([comment](https://github.com/pytorch/pytorch/pull/159678#issuecomment-3225597425))
- [Revert "[dynamo, nested graph breaks] support nested closures (#159817)"](https://github.com/pytorch/pytorch/commit/a7aa480e55ef5cdf56af53d93b381eca6680e2ae)
  - failing internal tests ([comment](https://github.com/pytorch/pytorch/pull/159817#issuecomment-3225586996))
- [Revert "[ROCm] SDPA fix mem fault when dropout is enabled (#154864)"](https://github.com/pytorch/pytorch/commit/9f6e1b8730d6a7a7d012be90ae08674294aa4933)
  - reverted internally ([comment](https://github.com/pytorch/pytorch/pull/154864#issuecomment-3225554119))
- [Revert "[dynamo, nested graph breaks] clean up comments and codegen (#160138)"](https://github.com/pytorch/pytorch/commit/caf98fde0d5c47452af45dc77099449edd521579)
  - failing internal tests ([comment](https://github.com/pytorch/pytorch/pull/160138#issuecomment-3225546707))
- [Revert "[dynamo, nested graph breaks] prevent excessive recompilations (#159786)"](https://github.com/pytorch/pytorch/commit/46576f5a164fcf95ec7fceaa13516bcb1ca4f6ab)
  - failing internal tests ([comment](https://github.com/pytorch/pytorch/pull/159786#issuecomment-3225535752))
- [Revert "Ensure large tensor int32 -> int64 indexing is enabled (#157767)"](https://github.com/pytorch/pytorch/commit/818ba434c7de4cd604184b2857d544e0ad95735f)
  - internal failure, sorry will revert ([comment](https://github.com/pytorch/pytorch/pull/157767#issuecomment-3224341111))
- [Revert "[dynamo] Refactor convert_frame.compile_frame to be self contained function. [5/n] (#160900)"](https://github.com/pytorch/pytorch/commit/e795450a35bca909902e12de99245e1c0e7e2872)
  - reverting since can't land existing diff internally, will need to reland it ([comment](https://github.com/pytorch/pytorch/pull/160900#issuecomment-3224029031))
- [Revert "[inductor] structured-log graph execution order + test (#160448)"](https://github.com/pytorch/pytorch/commit/4a1aca11c20cfa29a1513b9f289d75bfe32d05d4)
  - internal failure please see associated diff ([comment](https://github.com/pytorch/pytorch/pull/160448#issuecomment-3223939035))

### Ignored Signal (2)

- [Revert "Support Triton kernels in SAC region (#161541)"](https://github.com/pytorch/pytorch/commit/e9975f501cfb849d2cf470c09c9e2e0de42c9dc9)
  - Broke some tests in trunk workflow, see https://hud.pytorch.org/hud/pytorch/pytorch/main/1?per_page=50&name_filter=trunk%20%2F%20linux-jammy-cuda12.8 ([comment](https://github.com/pytorch/pytorch/pull/161541#issuecomment-3233457206))
- [Revert "Fix index_add for int64 input + zerodim index (#161511)"](https://github.com/pytorch/pytorch/commit/28af843ee0ea79867b7fd4ddc5bd0072d6518f3a)
  - broke test_indexing.py::TestIndexingCPU::test_index_add_zerodim_index_floating_alpha_cpu [GH job link](https://github.com/pytorch/pytorch/actions/runs/17257089116/job/48971728595) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/d51486616cb3fe54bc298669a88059be56c1fb22) on dynamo? ([comment](https://github.com/pytorch/pytorch/pull/161511#issuecomment-3228705842))

### Not through pytorchbot (6)

- [Revert "Refactor CUDAAllocatorConfig to reuse AcceleratorAllocatorConfig (#150312)" (#161628)](https://github.com/pytorch/pytorch/commit/b9c6aa1e17ed56d2d0b63d1ebde016449b5952ab)
- [Revert "Deprecate overleap functions in CUDAAllocatorConfig, use AcceleratorAllocatorConfig instead (#156165)" (#161627)](https://github.com/pytorch/pytorch/commit/b7b9fb9962457c871d1146a28214ed85efdeea82)
- [Revert "Generalize torch._C._set_allocator_settings to be generic (#156175)" (#161626)](https://github.com/pytorch/pytorch/commit/c03d8d4082b41c47be18e2699592a4a8b240cb31)
- [Revert "Back out "Deprecate overleap functions in CUDAAllocatorConfig, use AcceleratorAllocatorConfig instead (#156165)" (#160999)" (#161625)](https://github.com/pytorch/pytorch/commit/06ddaf1e0affe758cb6369f54680f359ef7ad6db)
- [Revert "[Dynamo] Allow inlining into AO quantization modules (#152934)" (#161567)](https://github.com/pytorch/pytorch/commit/be55d7ac9e9c6f070d1f8e2f387e4745d0fbee7c)
- [Back out "Refactor CUDAAllocatorConfig to reuse AcceleratorAllocatorConfig (#150312)" (#161002)](https://github.com/pytorch/pytorch/commit/a03cc53e6f6e2fe67316cb8c74c25f5b953f445b)

### No Signal (17)

- [Revert "Use vectorized stores for all dtypes (#161649)"](https://github.com/pytorch/pytorch/commit/e015de19695402569e2029429c10508f938b6f05)
  - buggy ([comment](https://github.com/pytorch/pytorch/pull/161649#issuecomment-3238895967))
- [Revert "[CI] Migrate XPU build and test to python 3.10 (#161708)"](https://github.com/pytorch/pytorch/commit/6e548c1a87906de3ac30f02dd2ea647f83eae87c)
  - Sorry but this is causing rocm jobs to fail. See: test/inductor/test_max_autotune.py::TestMaxAutotuneSubproc::test_max_autotune_addmm_search_space_EXHAUSTIVE_dynamic_True [GH job link](https://github.com/pytorch/pytorch/actions/runs/17303310877/job/49125664617) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/2a70d98abf8256d3d768eff028fca20198579824) ([comment](https://github.com/pytorch/pytorch/pull/161708#issuecomment-3238359944))
- [Revert "[MPS] sparse add unary funcs + add for sparse tensors (#160839)"](https://github.com/pytorch/pytorch/commit/f6368e934e6bef84211f7db82c22e3623038e43f)
  - test_sparse_csr.py::TestSparseCompressedCPU::test_consistency_SparseCSR_asinh_cpu_complex64 [GH job link](https://github.com/pytorch/pytorch/actions/runs/17329155095/job/49201551217) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/93c5112f46a978a029644ae599979416ead5c917) ([comment](https://github.com/pytorch/pytorch/pull/160839#issuecomment-3238093296))
- [Revert "[RELAND] Close some sources of fake tensor leakage (#161589)"](https://github.com/pytorch/pytorch/commit/9b67d8e34493d513129f344696fe0f21e00878ae)
  - [GH job link](https://github.com/pytorch/pytorch/actions/runs/17305150611/job/49128381649) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/5790b009751e6ebba35d3e6d05e7c1b135553eee) ([comment](https://github.com/pytorch/pytorch/pull/161589#issuecomment-3235224249))
- [Revert "kill allow_complex_guards_as_runtime_asserts (#160198)"](https://github.com/pytorch/pytorch/commit/47742081c9973b5b21f72aa5380ae8700cee38be)
  - let's revert again instead of waiting for forward fix, see earlier comments ([comment](https://github.com/pytorch/pytorch/pull/160198#issuecomment-3235165462))
- [Revert "Add ciflow/vllm to vLLM commit hash update PR(s) (#161678)"](https://github.com/pytorch/pytorch/commit/f46e4bcf43e3eccd857b57ae2d96e72043fc1fc9)
  - we want to keep the vllm pinn updated now, right now we have some failure ([comment](https://github.com/pytorch/pytorch/pull/161678#issuecomment-3234876332))
- [Revert "[dynamo] [guard] Add caching for inside torch.compile.disable function to avoid unnecessary recompilation. (#160934)"](https://github.com/pytorch/pytorch/commit/049c08eda8bd820a8514f5d785b10d4024bfd72b)
  - causes memory leak leading to OOMs ([comment](https://github.com/pytorch/pytorch/pull/160934#issuecomment-3234426359))
- [Revert "kill allow_complex_guards_as_runtime_asserts (#160198)"](https://github.com/pytorch/pytorch/commit/a8270dd1248569e667861b2a128417537228a899)
  - dynamo/test_activation_checkpointing.py::ActivationCheckpointingViaTagsTestsCUDA::test_compile_selective_checkpoint_triton_kernel_cuda [GH job link](https://github.com/pytorch/pytorch/actions/runs/17289619543/job/49074475338) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/196232bb935cb346f143d5c39e9a73c44121a033) ([comment](https://github.com/pytorch/pytorch/pull/160198#issuecomment-3234013520))
- [Revert "Add test coverage to tf32 in max autotune mm configs (#161545)"](https://github.com/pytorch/pytorch/commit/05d0f11dbdbaeb2ca89e93636eaeff125b5aefac)
  - inductor/test_max_autotune.py::TestMaxAutotuneRemoteCache::test_get_mm_configs_float32_precision_ieee [GH job link](https://github.com/pytorch/pytorch/actions/runs/17283985553/job/49058214260) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/e9d34b2438d65d6d16109e2416f3698de20f85c2) ([comment](https://github.com/pytorch/pytorch/pull/161545#issuecomment-3233569771))
- [Revert "[Inductor] Add DeviceAssert op to enable device-side assertion in torch.compile (#160677)"](https://github.com/pytorch/pytorch/commit/de585058904ae03abbd4d18086bf1842dd030546)
  - This is breaking tests on Rocm ([comment](https://github.com/pytorch/pytorch/pull/160677#issuecomment-3226541063))
- [Revert "Update pybind11 submodule to 3.0.1 (#160754)"](https://github.com/pytorch/pytorch/commit/1b34e044853e41d0ecc6c1d341a339931b76bc0b)
  - please see https://github.com/pytorch/pytorch/pull/160754#issuecomment-3226051449 ([comment](https://github.com/pytorch/pytorch/pull/160754#issuecomment-3226078102))
- [Revert "[cpp_wrapper] Swap to new PyBind11 simple GIL header (#161063)"](https://github.com/pytorch/pytorch/commit/1ce423274dd29c7fd783241cd43d8c6b407497ca)
  - sorry broke vllm tests please see https://github.com/pytorch/pytorch/pull/160754#issuecomment-3226051449 ([comment](https://github.com/pytorch/pytorch/pull/161063#issuecomment-3226065212))
- [Revert "[Inductor] Prune configs that require more shared memory than the hardware limit (#161040)"](https://github.com/pytorch/pytorch/commit/92ab18482459a63e97f1374e27e8411964da9762)
  - still failing on rocm, see https://hud.pytorch.org/failure?name=rocm%20%2F%20linux-jammy-rocm-py3.10%20%2F%20test%20(default%2C%203%2C%206%2C%20linux.rocm.gpu.2)&jobName=undefined&failureCaptures=inductor%2Ftest_triton_heuristics.py%3A%3ATestTritonHeuristics%3A%3Atest_prune_configs_over_shared_memory_limit_do_pruning_True ([comment](https://github.com/pytorch/pytorch/pull/161040#issuecomment-3222430129))
- [Revert "[dynamo] Refactor convert_frame.compile_frame to be self contained function. [5/n] (#160900)"](https://github.com/pytorch/pytorch/commit/3e210f90c2cbd5817aa23d430da10cad200a3ffa)
  - executorch failure ([comment](https://github.com/pytorch/pytorch/pull/160900#issuecomment-3221372096))
- [Revert "Fix conv exhaustive autotuning and expand Exhaustive test coverage (#159387)"](https://github.com/pytorch/pytorch/commit/df571ae7ad7dacf77ce42c00189cf369d7993387)
  - breaks ROCm, AttributeError: 'torch._C._CudaDeviceProperties' object has no attribute 'shared_memory_per_block_optin' ([comment](https://github.com/pytorch/pytorch/pull/159387#issuecomment-3220989480))
- [Revert "[AMD] Fix AMD User Defined Kernel Autotune (#160671)"](https://github.com/pytorch/pytorch/commit/40c0e700a488191cd8f541b30d8e3b9f2c0bc759)
  - new test is failing: inductor/test_aot_inductor.py::AOTInductorTestABICompatibleGpu::test_rocm_triton_autotuning_cuda [GH job link](https://github.com/pytorch/pytorch/actions/runs/17172795679/job/48725235301) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/431846a6323c6f1d02da49e311ac694324f386f4) ([comment](https://github.com/pytorch/pytorch/pull/160671#issuecomment-3220442141))
- [Revert "[inductor] Windows inductor use intel-openmp. (#160258)"](https://github.com/pytorch/pytorch/commit/ab7787fb82dd777b2f777ef58bc20dbb7bd8289b)
  - Reverting to fix https://github.com/pytorch/pytorch/issues/160898 and https://github.com/pytorch/pytorch/issues/160962 ([comment](https://github.com/pytorch/pytorch/pull/160258#issuecomment-3220158145))

### Weird (4)

- [Revert "Cleanup stale submodule directories after checkout (#161748)"](https://github.com/pytorch/pytorch/commit/6db872fa2c7f8ab01aed5bc91eb6a6f786b5719a)
  - I still see the same failures, and could not understand, from the log whether those checks are running on not ([comment](https://github.com/pytorch/pytorch/pull/161748#issuecomment-3238791895))
- [Revert "Cleanup stale submodule directories in checkout action (#161748)"](https://github.com/pytorch/pytorch/commit/823a329984c2fab59bf203d7c7cd1774e8b2ca3a)
  - I put the check in the wrong place ([comment](https://github.com/pytorch/pytorch/pull/161748#issuecomment-3237080419))
- [Revert "Increase timeout value when pushing to ghcr.io (#161444)"](https://github.com/pytorch/pytorch/commit/908b0ccb1f70ed2cfa830484e05ee32af13b1836)
  - Reland this to generate a different has value for the benchmark Docker image ([comment](https://github.com/pytorch/pytorch/pull/161444#issuecomment-3222257119))
- [Revert "[BE] Remove intel-openmp dependency in setup.py (#160976)"](https://github.com/pytorch/pytorch/commit/1eccfb157ab9855b3f81872a23502fb15f455e0a)
  - This PR is doing something strange ([comment](https://github.com/pytorch/pytorch/pull/160976#issuecomment-3220120462))
