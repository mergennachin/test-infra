# Week of 2025-09-01 to 2025-09-08 (42)

### GHFirst (21)

- [Revert "[2/N]Port several test files under test/distributed to Intel GPU (#159473)"](https://github.com/pytorch/pytorch/commit/ff2de5d52236f98f3fffbce295842eb81db4203d)
  - Seems to be breaking internal signals, @d4l3k please help the author to have this change landed. [D81718444](https://www.internalfb.com/diff/D81718444) ([comment](https://github.com/pytorch/pytorch/pull/159473#issuecomment-3264046983))
- [Revert "[BE] Cleanup stale comments/copy from `gemm`  (#162001)"](https://github.com/pytorch/pytorch/commit/df59c21768047cc37882a033fd20b184e0a0e798)
  - breaks internal ads signal, see [D81845017](https://www.internalfb.com/diff/D81845017) ([comment](https://github.com/pytorch/pytorch/pull/162001#issuecomment-3264034312))
- [Revert "[inductor] add kernel template choice (ktc) (#161347)"](https://github.com/pytorch/pytorch/commit/093ab5f477586915acd1d376d5fd91f9e57626b0)
  - Seems to have broken internal builds, see [D81520569](https://www.internalfb.com/diff/D81520569) ([comment](https://github.com/pytorch/pytorch/pull/161347#issuecomment-3264027436))
- [Revert "[inductor][ez] V.choices.get_mm_configs returns list of ChoiceCallers (#161348)"](https://github.com/pytorch/pytorch/commit/4348db0b92c5260dae032ca2d192d5eb8e9a4c14)
  - Seems to have broken internal builds, see [D81520569](https://www.internalfb.com/diff/D81520569) ([comment](https://github.com/pytorch/pytorch/pull/161347#issuecomment-3264027436))
- [Revert "[inductor] pdl inductor option (disabled by default) (#160928)"](https://github.com/pytorch/pytorch/commit/ada43ed39c80b746b4822c92640a1882619e2795)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/160928#issuecomment-3263560378))
- [Revert "Always build USE_DISTRIBUTED. (#160449)"](https://github.com/pytorch/pytorch/commit/adae7f66aacf3f248c3101b858cf98d5809119fa)
  - Breaking internal build rules, see D81756619 ([comment](https://github.com/pytorch/pytorch/pull/160449#issuecomment-3259430011))
- [Revert "Make distributed modules importable even when backend not built (#159889)"](https://github.com/pytorch/pytorch/commit/70f865ac9bcd8a38e9e35d27d5a72de494f1a8b5)
  - Breaking internal build rules, see D81756619 ([comment](https://github.com/pytorch/pytorch/pull/160449#issuecomment-3259430011))
- [Revert "Rename propagate_tensor_meta to make private again (#161744)"](https://github.com/pytorch/pytorch/commit/f3cebec39ebc110e1c8b06e741896585f7892dbb)
  - seems to break internal tests, see D81657000 for more details ([comment](https://github.com/pytorch/pytorch/pull/161744#issuecomment-3258934519))
- [Revert "[BLAS] Avoid downcasts for fp16fp16->fp32 BLAS (#161999)"](https://github.com/pytorch/pytorch/commit/c3d54dea9febb1236d48d19e5d4876a63f2e20fd)
  - break a few internal tests ([comment](https://github.com/pytorch/pytorch/pull/161999#issuecomment-3255381925))
- [Revert "[BE] Cleanup stale comments/copy from `gemm`  (#162001)"](https://github.com/pytorch/pytorch/commit/afa6e5604d78b447aca3e30d9843732c1ee26885)
  - break a few internal tests ([comment](https://github.com/pytorch/pytorch/pull/161999#issuecomment-3255381925))
- [Revert "[MPS] enable cat op for sparse (#162007)"](https://github.com/pytorch/pytorch/commit/9e5247f51d81735e5f1e65e80588985fa93bccc5)
  - Breaks internal builds see [D81588372](https://www.internalfb.com/diff/D81588372), @malfet may you help the author? ([comment](https://github.com/pytorch/pytorch/pull/162007#issuecomment-3255357336))
- [Revert "Always build USE_DISTRIBUTED. (#160449)"](https://github.com/pytorch/pytorch/commit/b7dad7dd49448c88d0751fa2e29c70afe985f734)
  - Already discussed with @ezyang about the internal quirks and errors ([comment](https://github.com/pytorch/pytorch/pull/160449#issuecomment-3254219358))
- [Revert "Make distributed modules importable even when backend not built (#159889)"](https://github.com/pytorch/pytorch/commit/34aa78274d6770086025a967fa63a86830e08176)
  - Failing internal tests, probably typechecks. See D81588399 ([comment](https://github.com/pytorch/pytorch/pull/159889#issuecomment-3253651785))
- [Revert "[inductor][ez] add hook for heuristics to adjust kernel input nodes (#161339)"](https://github.com/pytorch/pytorch/commit/bb950284c7e72905994bc25dd436c10e48088d85)
  - Breaks internal tests, check D81520572 for more details ([comment](https://github.com/pytorch/pytorch/pull/161339#issuecomment-3249600885))
- [Revert "[CUDAGraph] add config to error on skipping cudagraph (#161862)"](https://github.com/pytorch/pytorch/commit/f27985b7e796fb66a1b476284ba42d8cb360a751)
  - Breaks internal tests, see D81522732 for more details ([comment](https://github.com/pytorch/pytorch/pull/161862#issuecomment-3249582583))
- [Revert "test: ensure editable cached wrapper is respected (#160943)"](https://github.com/pytorch/pytorch/commit/0cd6c56bdfa9178ff61be82ce3b178926ddb64a9)
  - See [D81486248](https://www.internalfb.com/diff/D81486248) for details on broken test ([comment](https://github.com/pytorch/pytorch/pull/160943#issuecomment-3249565671))
- [Revert "Always build USE_DISTRIBUTED. (#160449)"](https://github.com/pytorch/pytorch/commit/4e42aa8ffc44b8340eb0eeaf80a2cafc4763a186)
  - Breaking internal builds, can't be landed with forward fix due to internal tooling problems ([comment](https://github.com/pytorch/pytorch/pull/160449#issuecomment-3246689684))
- [Revert "Make distributed modules importable even when backend not built (#159889)"](https://github.com/pytorch/pytorch/commit/420c52ecf36f86d32da0853bfbe074b682b070aa)
  - Breaking internal builds, can't be landed with forward fix due to internal tooling problems ([comment](https://github.com/pytorch/pytorch/pull/159889#issuecomment-3246677982))
- [Revert "[HOTFIX] Disable DISTRIBUTED_C10D_DIRECT_ACCESS for now (#161946)"](https://github.com/pytorch/pytorch/commit/82f63c8f6de63c30132a8ac299b6e8c2fd0d3fe8)
  - Need to be reverted so https://github.com/pytorch/pytorch/pull/159889 can be ([comment](https://github.com/pytorch/pytorch/pull/161946#issuecomment-3246663376))
- [Revert "Fix conv exhaustive autotuning and expand Exhaustive test coverage (#159387)"](https://github.com/pytorch/pytorch/commit/17fa8eec4a1e32939ab4d364ee6e75487a79b654)
  - need to revert due to merge conflicts, please feel free to merge it back in once conflicts are resolved ([comment](https://github.com/pytorch/pytorch/pull/159387#issuecomment-3242945661))
- [Revert "[Fix XPU CI][Inductor UT] Fix test cases broken by community. (#161142)"](https://github.com/pytorch/pytorch/commit/54e275e0d81fe1e1ccfa4fb5f2a5a9aaca00ca15)
  - This PR needs to be reverted to be able to revert another PR, this is due to merge conflicts, I am sorry for this. Please feel free to rebase and merge at your earliest convenience ([comment](https://github.com/pytorch/pytorch/pull/161142#issuecomment-3242937640))

### Ignored Signal (3)

- [Revert "Fix Arm64 OSS pytorch build with FBGEMM (#161527)"](https://github.com/pytorch/pytorch/commit/1ec2c15914da4ef7bd926ed9aebc8671c75fe965)
  - This breaks all Mac builds, see https://hud.pytorch.org/hud/pytorch/pytorch/b04e922712080a3652e438d05e8bb74e0cd2d238/1?per_page=50&name_filter=macos ([comment](https://github.com/pytorch/pytorch/pull/161527#issuecomment-3256034443))
- [Revert "[SymmMem] Add root argument to broadcast op (#161090)"](https://github.com/pytorch/pytorch/commit/d5b38410b5b6cf75c7a7389972777a6497926ee7)
  - breaks internal builds ([comment](https://github.com/pytorch/pytorch/pull/161090#issuecomment-3255574093))
- [Revert "Fix usage of forwarding references (#161094)"](https://github.com/pytorch/pytorch/commit/48bedd753da22634aa94fbafeb731e82025404f3)
  - checking if revert will fix https://github.com/pytorch/pytorch/actions/runs/17470601839/job/49621447581 ([comment](https://github.com/pytorch/pytorch/pull/161094#issuecomment-3255541480))

### No Signal (16)

- [Revert " [while_loop][autograd] support autograd_key of while_loop (#160483)"](https://github.com/pytorch/pytorch/commit/7a83cf430e97d83d6fb14880b9049e77ff725685)
  - Sorry for reverting your PR, but some trunk tests are failing either from this PR or the previous one in the stack ([comment](https://github.com/pytorch/pytorch/pull/160483#issuecomment-3263597325))
- [Revert "Add return-max-scores to flex-attention (#161667)"](https://github.com/pytorch/pytorch/commit/104f2680e03d13a4765ca69f905d8f16fc0c822f)
  - Sorry for reverting your change but reverting https://github.com/pytorch/pytorch/pull/161730 does not seem to fix all trunk failures ([comment](https://github.com/pytorch/pytorch/pull/161667#issuecomment-3263512642))
- [Revert "[inductor] fuse for scalar shared data (#162311)"](https://github.com/pytorch/pytorch/commit/eac3d6f04cfbbebe3d470dacd216da7d4b1f95a8)
  - Sorry for reverting your change, but it is breaking lint ([comment](https://github.com/pytorch/pytorch/pull/162311#issuecomment-3263511162))
- [Revert "[dynamo] Graph break on on user-defined class in compiled region (#161670)"](https://github.com/pytorch/pytorch/commit/0ff8eabf1387de5acd6712a03bda61f1a3dfa27f)
  - seems to have introduced https://github.com/pytorch/pytorch/actions/runs/17507127561/job/49733379267 and https://github.com/pytorch/pytorch/actions/runs/17507127561/job/49733379271 ([comment](https://github.com/pytorch/pytorch/pull/161670#issuecomment-3261241229))
- [Revert "Resize to 0 if not going to be used (#161730)"](https://github.com/pytorch/pytorch/commit/a3e5466002791da609fcb069155d8ee347baee92)
  - functorch/test_aotdispatch.py::TestAOTModuleSimplified::test_flex_attn_noncontiguous_tangents [GH job link](https://github.com/pytorch/pytorch/actions/runs/17506617662/job/49731934012) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/081cab045472ce045634548cc6c14a4870641e23) ([comment](https://github.com/pytorch/pytorch/pull/161730#issuecomment-3260492575))
- [Revert "[ROCm] [CK] Composable Kernel integration for inductor backend (#158747)"](https://github.com/pytorch/pytorch/commit/d711f27845abd45007ccab6076649ebd896c2661)
  - Broke linux-binary-manywheel-rocm / manywheel-py3_9-rocm6_4-test: https://hud.pytorch.org/hud/pytorch/pytorch/019fed39aa6b2dd8c69347378d53423e5efae8d4/1?per_page=50&name_filter=manywheel&mergeEphemeralLF=true ... PR didn't have this job run successfully due to CI outage ([comment](https://github.com/pytorch/pytorch/pull/158747#issuecomment-3259212343))
- [Revert "[nativert] triton runtime implementation (#161798)"](https://github.com/pytorch/pytorch/commit/95ee0bfea99d3d346d6502b91b497d2b35795504)
  - introducing linting failures ([comment](https://github.com/pytorch/pytorch/pull/161798#issuecomment-3255412085))
- [Revert "[ROCm] Use MI325 (gfx942) runners for binary smoke testing (#162044)"](https://github.com/pytorch/pytorch/commit/6b8b3ac4403f771bd4a8f9a45d93347304148774)
  - mi200 backlog is purged, and mi300 runners are failing in GHA download ([comment](https://github.com/pytorch/pytorch/pull/162044#issuecomment-3254427869))
- [Revert "Contiguous subgraph decomposition (#161241)"](https://github.com/pytorch/pytorch/commit/aad96a202244c7d0d120c04ba8db593edd8c0f92)
  - breaks rocm mi300 tests ([comment](https://github.com/pytorch/pytorch/pull/161241#issuecomment-3251185098))
- [Revert "Update Kineto submodule (#161572)"](https://github.com/pytorch/pytorch/commit/4cdaf8265d86f984254b62052da8c26ef61ef1cf)
  - This appears as though its causing downstream build failures in inductor workflows and for developers working locally. Going to revert out of an abundance of caution. ([comment](https://github.com/pytorch/pytorch/pull/161572#issuecomment-3247121981))
- [Revert "Add inductor provenance mapping for cpp extern kernel (#161656)"](https://github.com/pytorch/pytorch/commit/15c77a8cfd341e74fd124b077492ef2bfa51b339)
  - causing failures on ROCm MI300, will add label to PR ([comment](https://github.com/pytorch/pytorch/pull/161656#issuecomment-3246965676))
- [Revert "Add __init__.pyi to torch/linalg (#160750)"](https://github.com/pytorch/pytorch/commit/d6b74568e2c98ce58ecc145b72ac66d4caf7ce95)
  - Seems that those errors are legitimate, and there is no test plan. I'll be proceeding with a revert ([comment](https://github.com/pytorch/pytorch/pull/160750#issuecomment-3246095383))
- [Revert "[BE] Update xpu driver repo for CD used almalinux 8.10 (#157356)"](https://github.com/pytorch/pytorch/commit/e304ea4e69d3a7deeb7e48c7450c214a4c953937)
  - This PR has performance regression on some workloads ([comment](https://github.com/pytorch/pytorch/pull/157356#issuecomment-3245319046))
- [Revert "Defer loading hipify until it is needed (#160824)"](https://github.com/pytorch/pytorch/commit/13b65196db422bdb394cb482e208c61ed448898c)
  - Broke slow tests test_utils.py::TestHipifyTrie::test_special_char_export_trie_to_regex [GH job link](https://github.com/pytorch/pytorch/actions/runs/17387051351/job/49355619371) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/403a3a393cda7e60f503f3b04b8805a845dcf45d) ([comment](https://github.com/pytorch/pytorch/pull/160824#issuecomment-3243281628))
- [Revert "[CUDA] Reuse blocks with record_stream during CUDA Graph capture in the CUDACachingAllocator (#158352)"](https://github.com/pytorch/pytorch/commit/63a9c23fe99eacfd09610c36dfe8f01b053c1a35)
  - Broke cuda 13.0 nightly builds https://github.com/pytorch/pytorch/actions/runs/17382188549/job/49341981474 ([comment](https://github.com/pytorch/pytorch/pull/158352#issuecomment-3242871629))
- [Revert "[cuBLASLt][FP8] `cuBLASLt` appears to support float8 rowwise-scaling on H100 (#161305)"](https://github.com/pytorch/pytorch/commit/21fae99c180d17def562797ea0fb154d8fdf88e3)
  - Broke test_matmul_cuda.py::TestFP8MatmulCUDA::test_float8_error_messages_cuda [GH job link](https://github.com/pytorch/pytorch/actions/runs/17309011599/job/49140215634) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/1190b7f73e9a94c9280d2baf196fddaa4c3a0374) ([comment](https://github.com/pytorch/pytorch/pull/161305#issuecomment-3242652672))

### Weird (2)

- [Revert "[ROCm] Enabling several UTs (#161715)"](https://github.com/pytorch/pytorch/commit/8235c4f65d881a61c20a6a9bd1f5be4941215361)
  - Need to revert in order to revert https://github.com/pytorch/pytorch/pull/159473, feel free to merge it back once conflicts are cleared ([comment](https://github.com/pytorch/pytorch/pull/161715#issuecomment-3264040604))
- [Revert "[1/N] Port 5 _composable/fsdp distributed test cases to Intel GPU (#159118)"](https://github.com/pytorch/pytorch/commit/e246a85b768fe12ff485f626fa9d7e876a36a645)
  - Need to revert in order to revert https://github.com/pytorch/pytorch/pull/159473, feel free to merge it back once conflicts are cleared ([comment](https://github.com/pytorch/pytorch/pull/159118#issuecomment-3264037799))
