# Week of 2025-05-19 to 2025-05-26 (27)

### GHFirst (9)

- [Revert "Patch the _is_conv_node function (#153749)"](https://github.com/pytorch/pytorch/commit/561a11aa68cd5b3bf6ea817931f64d33fddc7608)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/153749#issuecomment-2905504697))
- [Revert "Update the heuristic for AArch64 bmm/baddbmm (#149122)"](https://github.com/pytorch/pytorch/commit/866142ff166d8e83ac6681911e7ab5124481c49e)
  - breaking internal models, @malfet may you help merge this? ([comment](https://github.com/pytorch/pytorch/pull/149122#issuecomment-2904703075))
- [Revert "[DDP] rebuilt bucket order when find_unused_parameters=true (#153404)"](https://github.com/pytorch/pytorch/commit/59c5fff2aa2216612ea54e287a73f2055aab84ac)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/153404#issuecomment-2902741300))
- [Revert "[inductor][cutlass backend] Add 2 stage autotuning aka prescreening (#153335)"](https://github.com/pytorch/pytorch/commit/025c5cc048358d7d5675c8142ae0249ba6c4c627)
  - sorry the pr is failed internally [D75155648](https://www.internalfb.com/diff/D75155648) ([comment](https://github.com/pytorch/pytorch/pull/153335#issuecomment-2901916364))
- [Revert "[BE]: Type previously untyped decorators (#153726)"](https://github.com/pytorch/pytorch/commit/7d3dab6b9016a42038a55798317bf43dfcc07ad0)
  - sorry, it seems like your pr failed typecheck error internally, [D75155486](https://www.internalfb.com/diff/D75155486) ([comment](https://github.com/pytorch/pytorch/pull/153726#issuecomment-2901911114))
- [Revert "[aoti] Add MPS runner and shim (#153964)"](https://github.com/pytorch/pytorch/commit/a82c8891d56064bd7cf8379e2896d89a1a3a780f)
  - broke frl build ([comment](https://github.com/pytorch/pytorch/pull/153964#issuecomment-2901876832))
- [Revert "[aoti] Initial Metal support (#153959)"](https://github.com/pytorch/pytorch/commit/47a01f3efb44c7d311318a004a08b406a3b31344)
  - previous PR broke frl build ([comment](https://github.com/pytorch/pytorch/pull/153959#issuecomment-2901825315))
- [Revert "Fixed an issue with XPU skip so the test_decompose_mem_bound_mm.py suite can be ran correctly (#153245)"](https://github.com/pytorch/pytorch/commit/500a710422f8b587f154dcfe9c4fe860ef50fef1)
  - tests failed internally [D75078034](https://www.internalfb.com/diff/D75078034) ([comment](https://github.com/pytorch/pytorch/pull/153245#issuecomment-2895785642))
- [Revert "Improve torch.ops typing (#153558)"](https://github.com/pytorch/pytorch/commit/d81217be2e4cc76ed11ff284de2f7639c2e3a383)
  - Your diff will not be landed to fbcode since we suspect it caused the following breakage in an internal test:[D75007157](https://www.internalfb.com/diff/D75007157) for instance: tests_gpu/lookup_gpu_index_test.py:232:8 Undefined attribute [16]: torch._ops._OpNamespace has no attribute simple_index_mm_batch ([comment](https://github.com/pytorch/pytorch/pull/153558#issuecomment-2892506789))

### Ignored Signal (1)

- [Revert "[AOTI][cutlass backend] Do not remove the cutlass kernel .o file after packaging (#154155)"](https://github.com/pytorch/pytorch/commit/90855835ffc3aac5965a5370868b8e4cc5d04d75)
  - mistake in PR ([comment](https://github.com/pytorch/pytorch/pull/154155#issuecomment-2905514934))

### No Signal (13)

- [Revert "[c10d] Add support for testing SIGABRT return (#153167)"](https://github.com/pytorch/pytorch/commit/54932d865e5883882f424b53738049db5c3000bb)
  - It broke lint ([comment](https://github.com/pytorch/pytorch/pull/153167#issuecomment-2907820789))
- [Revert "[c10d] Add support for testing SIGABRT return (#153167)"](https://github.com/pytorch/pytorch/commit/28af44285bfa0aabdbe7190efca4ae99205b2ce0)
  - Broke lint, see https://hud.pytorch.org/hud/pytorch/pytorch/fe784c5a2cea1a811d2489a2d3c635c14e4253fe/1?per_page=50&name_filter=lintrunner-noclang&mergeEphemeralLF=true ([comment](https://github.com/pytorch/pytorch/pull/153167#issuecomment-2905623868))
- [Revert "[export] Move PT2ArchiveWriter/Reader to torch/export (#153795)"](https://github.com/pytorch/pytorch/commit/4ff19ecf665dc2a8583e652b3e01c7d8ec135457)
  - Looks like it broke lots of tests, see https://hud.pytorch.org/hud/pytorch/pytorch/ec368a19036dfa42216b05216bc0400388aee50b/1?per_page=50&mergeEphemeralLF=true ([comment](https://github.com/pytorch/pytorch/pull/153795#issuecomment-2905415496))
- [Revert "cpp_wrapper: build non-performance-sensitive code at O1 (#148773)"](https://github.com/pytorch/pytorch/commit/261897734a4d3c0be31eceb20f14e9e1c2cd9644)
  - Sorry for reverting your change but it seems that pr_time_benchmark is regressed after this land ([comment](https://github.com/pytorch/pytorch/pull/148773#issuecomment-2899545140))
- [Revert "[3/n][Optimus][Auto-AC] Support float8_e4m3fn quantization type and set scaling as the default (#153802)"](https://github.com/pytorch/pytorch/commit/3eb8fa081a808a3a8c9b202a938e57d01d0b0b5b)
  - It breaks ROCM testing, see https://hud.pytorch.org/hud/pytorch/pytorch/d23762974eae105aad837188d5d2254ea9783b37/1?per_page=50&name_filter=rocm%20%2F%20linux-jammy&mergeEphemeralLF=true ([comment](https://github.com/pytorch/pytorch/pull/153802#issuecomment-2898695702))
- [Revert "[cuBLAS][cuBLASLt] Use cuBLAS default workspace size in Lt (#153556)"](https://github.com/pytorch/pytorch/commit/531d8f5fb6ea6e1fcac011e5af09067303839750)
  - reverting, will add disable for reduced precision reduction ([comment](https://github.com/pytorch/pytorch/pull/153556#issuecomment-2896257521))
- [Revert "[CI][CUDA] Move cu118 distributed pull jobs to cu126, move cu124-sm75 to cu126-sm75 (#151594)"](https://github.com/pytorch/pytorch/commit/1478d0185c291c9fd4b4a0beb604aa1a0fa94a2b)
  - Sorry for reverting your change but it seems to fail a distributed test in trunk ([comment](https://github.com/pytorch/pytorch/pull/151594#issuecomment-2896230131))
- [Revert "[cuBLASLt] relax `addmm` cuBLASLt constraint (#153675)"](https://github.com/pytorch/pytorch/commit/cf6e5d18818da43c0a239fcbba8d664cbf41a7ef)
  - incorrect, cuBLASLt doesnt handle beta != 1.0 but this appears untested ([comment](https://github.com/pytorch/pytorch/pull/153675#issuecomment-2896188784))
- [Revert "[inductor][cutlass backend] Add 2 stage autotuning aka prescreening (#153335)"](https://github.com/pytorch/pytorch/commit/7b7604fdb4efae84c5a177c610df98777c379128)
  - Breaks lint, see https://hud.pytorch.org/hud/pytorch/pytorch/3742b7fb3ab677cdfd4294a0b683044b67f4c358/1?per_page=50&name_filter=lintrunner&mergeEphemeralLF=true ([comment](https://github.com/pytorch/pytorch/pull/153335#issuecomment-2896031661))
- [Revert "[AOTI] Add an option to specify custom op C shim (#153851)"](https://github.com/pytorch/pytorch/commit/3102ae6798607f380fddd03bb55df375e5ed044d)
  - Looks like it broke fuzzer test, but I could be wrong, see https://hud.pytorch.org/hud/pytorch/pytorch/c4d1ff02f8ddec38a70118fa874ef39e061fa8cd/1?per_page=50&mergeEphemeralLF=true ([comment](https://github.com/pytorch/pytorch/pull/153851#issuecomment-2894619773))
- [Revert "Cache code generation during triton template expansion and enable it  for mm_template. (#151773)"](https://github.com/pytorch/pytorch/commit/b15720118a82bd05ae3df7e8810e2aad7992254f)
  - It broke ROCm, see https://hud.pytorch.org/hud/pytorch/pytorch/f9aa3bae8cec28528de819863b04eae75089c4c5/1?per_page=50&name_filter=rocm%20%2F%20linux-jammy&mergeEphemeralLF=true ([comment](https://github.com/pytorch/pytorch/pull/151773#issuecomment-2892587039))
- [Revert "[Distributed][CI] Rework continuous TestCase (#153653)"](https://github.com/pytorch/pytorch/commit/674a85cf26343b29b5cfbff61ec6a5afe8e62fc7)
  - More fixes needed ([comment](https://github.com/pytorch/pytorch/pull/153653#issuecomment-2891931028))
- [Revert "Recheck autotune cache on static cuda launcher load (#153565)"](https://github.com/pytorch/pytorch/commit/b0e5402377c0296c45ca7ae8a944427616521604)
  - Looks like it broke ROCM, see https://hud.pytorch.org/hud/pytorch/pytorch/ee72c53c884ce5d0cbdd50641557df5c5783afbf/1?per_page=50&name_filter=rocm%20%2F%20linux&mergeEphemeralLF=true ([comment](https://github.com/pytorch/pytorch/pull/153565#issuecomment-2891673913))

### Weird (4)

- [Revert "Fix fake tensor caching when output has unbacked (#153034)"](https://github.com/pytorch/pytorch/commit/1075bb37d34e483763a09c7810790d5491441e13)
  - Seems to have introduced flakiness in MacOS inductor tests, see https://github.com/pytorch/pytorch/issues/153891 ([comment](https://github.com/pytorch/pytorch/pull/153034#issuecomment-2893059329))
- [Revert "FakeTensorMode dispatch shouldn't include bypass in exception context (#153780)"](https://github.com/pytorch/pytorch/commit/9849c79fa21884ab89144bac86d4f40862cf0af0)
  - Reverting to clearly revert https://github.com/pytorch/pytorch/pull/153034, that seems to have introduced flakiness in MacOS inductor tests, see https://github.com/pytorch/pytorch/issues/153891 ([comment](https://github.com/pytorch/pytorch/pull/153780#issuecomment-2893053304))
- [Revert "[CI] Reuse old whl (#153838)"](https://github.com/pytorch/pytorch/commit/8c40c9ffcb49db075eeb4705d55ca057776d70f4)
  - testing on main is hard ([comment](https://github.com/pytorch/pytorch/pull/153838#issuecomment-2892272494))
- [Revert "[CI] Reuse old whl (#153838)"](https://github.com/pytorch/pytorch/commit/1ccacc028db1ffb9715c360a00a793c6daa2b90d)
  - forgot to comment some stuff out ([comment](https://github.com/pytorch/pytorch/pull/153838#issuecomment-2892195387))
